\section{Related Work}
Past approaches to combined audio and text modalities have their origin in voicemail systems, which notoriously favor the producer over the listener. 
To remedy this, Whittaker et al. \cite{whittaker} attempted to create a more balanced, usable system by converting the audio messages into text using ASR for the recipients. 
In that study, the combination of text and audio was considerably more appealing to listeners since it allowed them to extract information from messages more efficiently. 

Other related techniques for audio communication can be found in systems developed for document-based collaboration. 
Elaborating on the functionality of earlier platforms for multimedia annotation, such as Microsoft Research's MRAS software \cite{bargeron,mras}, an application called RichReview developed by Yoon et al. \cite{yoon} facilitates discussions on PDF documents via text, audio, or handwritten annotations. 
Effective use of AAC in these document-based scenarios is especially relevant since it allows commentators to engage in richer discussions over the material without visually distracting from the original content. 

In addition to increasing the efficiency of audio consumption, ASR can also be used to facilitate speech editing. 
Phoneme alignment algorithms such as the Penn Phonetics Lab Forced Aligner \cite{p2fa} align speech to the words in a transcript, enabling the use of text editing as a \emph{proxy} for lower-level audio manipulations.
Creating effective user interface paradigms that synthesize these two modalities, however, is a challenge that has been addressed mostly in dedicated audio and video editing software. 
For instance, a video editor developed by Casares et al. \cite{casares} enables the user to edit either the audio track or its transcript side-by-side, with changes in one mode reflected in the other. 
A similar approach was utilized by Rubin et al. \cite{rubin}, who recently created a text-based editor for ``audio stories'' containing both speech and music.

In software targeted for non-professional users, Whittaker and Amento \cite{whittaker_semantic} developed a ``semantic speech editor" in which the sole means of editing was through the transcription. 
They found that using the transcription as a proxy for audio editing was faster and produced better-quality revisions than audio-only modes, noting that the ability to scan quickly for regions that required editing obviated the need for repeated playback. 
However, the ASR in their 2004 study suffered from a 28\% error rate; today's language models permit considerably greater accuracy, resulting in potentially even easier and faster audio editing.