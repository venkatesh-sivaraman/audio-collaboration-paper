\section{Related Work}

The linear, sequential nature of voice communication not only impedes skimming and navigation capabilities \cite{grudin}, but can also hamper the speech \emph{production} process. 
Voice production is a temporarily linear process which demands that the speaker thinks and speaks simultaneously \cite{marriott2002, yoon:2015}.
Therefore, cognitive load arises from the fact that one has to keep speaking to prevent undesirable long pauses. 
In addition, mistakes in recorded speech are harder to revise than textual typos, mainly due to the lack of lightweight voice editing software \cite{marriott2002}.  
Building on the qualitative implications of these previous works, our study presents a quantitative analysis of these burdens when the voice production system includes lightweight editing features.

Because lower-level audio waveform editing is an onerous task, speech manipulation tools have been developed that present audio in semantically meaningful higher-level chunks, such as words and phrases.
Acoustic detection of the presence of speech provides binary visual guidance through which users can edit or index the speech recording \cite{ades1986, hindus:1992}; on the other hand, a pure acoustic approach has limited recognition granularity.
Time-aligned automatic speech recognition (ASR) has now become a popular tool to achieve the word-level structuring of speech \cite{Schmandt81, Wilcox:1992}. 
Compared with acoustic structuring, ASR presents semantic information at higher resolution, but has also suffered from high computational load and delay. 
However, recent technical developments have made ASR faster and more accurate, and we take full benefit of this real-time transcription capability.

Since speech transcription elicits the contents of the recording, researchers have utilized it to assist in visual skimming and navigation. 
MedSpeak \cite{Lai:1997} and SCANMail \cite{whittaker} are well recognized as precursors of such systems that use time-alignment data of the transcript for indexing audio. 
Since transcription errors tend to obstruct visual comprehension, Vemuri et al. suggested a novel visualization of the transcript that adjusts transcription brightness to the word's ASR confidence score \cite{Vemuri:2004}. 

On the production side, there have been several systems that use a time-aligned transcript for editing audio \cite{rubin,whittaker_semantic,yoon} or video \cite{Berthouzoz:2012,casares}. 
Among them, Whittaker and Rubin's editing system leveraged users' familiarity with text-editing interfaces to adopt audio editing within that paradigm. 
Since we targeted non-professional users, our interface also took the text-like approach, but emphasizing a \emph{live} production process and going beyond editing previously-transcribed speech. 

Just as with listeners, ASR errors can be detrimental for understanding and skimming audio contents during editing \cite{halverson1999beauty}. 
In the MedSpeak interface, Lai et al. provided a separate graphical window for fixing transcription errors \cite{Lai:1997}. 
In a speech production system like SimpleSpeech, though, users could easily get lost between the audio editing and transcription correction modes, so we chose to guide the user's attention through these modes via the movement of the editing caret into a quasi-modal interface.

Pauses in speech deliver nuanced meaning such as hesitation or emphasis, so easy and powerful manipulation of pause duration is important. 
A system called SpeechSkimmer automatically condenses pauses for fast auditory skimming \cite{arons:1993}. 
Other previous systems supported pause editing via a designated button \cite{Berthouzoz:2012} or specialized tags \cite{rubin}. 
Rubin et al.'s system used the period key as a shortcut to insert pause tags, but the duration of the gap was preset and required the use of a separate menu. 
Our approach is based on a traditional text editor, where users adjust lengths of pauses by adding and deleting spaces with the space bar and delete keys. The length of the pause corresponds to the number of spaces.

Perhaps most importantly, studies of the mental and linguistic effects of AAC discourse are limited despite the variety of AAC system designs discussed above.
Those studies that exist are focused on the larger category of computer-mediated communication (CMC), which is dominated by textual media such as SMS, email, or Facebook posts.
For example, Kiesler, Siegel, and McGuire \cite{kiesler} found more equalized group participation and more uninhibited expression of opinions in synchronous text-based CMC than in face-to-face discussions.
Asynchronous CMC, similar to a discussion board, induces more prosocial behavior and, in fact, more informal communication styles over time than face-to-face \cite{walther}.
On the other hand, formality and politeness in emails has been shown to increase as the social distance, status gap, and importance of a request increase \cite{cho}.
How these findings about textual media relate to spoken modes of CMC has not been heavily investigated, however.
