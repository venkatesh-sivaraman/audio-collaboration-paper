\documentclass{sigchi}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.


%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
% \toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is      granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)


% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy 

%\pagenumbering{arabic}

% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead 
%\usepackage[T1]{fontenc}
\usepackage{txfonts}
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage[pdftex]{hyperref}
% \usepackage{url}      % llt: nicely formatted URLs
\usepackage{color}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{ccicons}
\usepackage{todonotes}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={Editability in Online Audio Discussion},
  pdfauthor={LaTeX},
  pdfkeywords={SIGCHI, proceedings, archival format},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{Editability in Online Audio Discussion}

\numberofauthors{3}
\author{%
  \alignauthor{Venkatesh Sivaraman\\
    \affaddr{Bexley High School}\\
    \affaddr{Columbus, OH, USA}\\
    \email{venkats@mit.edu}}\\
  \alignauthor{Dongwook Yoon\\
    \affaddr{Cornell University}\\
    \affaddr{Ithaca, NY, USA}\\
    \email{dy252@cornell.edu}}\\
  \alignauthor{Piotr Mitros\\
    \affaddr{edX, Inc.}\\
    \affaddr{Cambridge, MA, USA}\\
    \email{pmitros@edx.org}}\\
}

\maketitle

\begin{abstract}
  UPDATED---\today. This sample paper describes the
  formatting requirements for SIGCHI conference proceedings, and
  offers recommendations on writing for the worldwide SIGCHI
  readership. Please review this document even if you have submitted
  to SIGCHI conferences before, as some format details have changed
  relative to previous years. Abstracts should be about 150 words and
  are required.
\end{abstract}

\keywords{Authors' choice; of terms; separated; by semi\-colons;
  commas, within terms only; this section is required.}

\category{H.5.m.}{Information Interfaces and Presentation
  (e.g. HCI)}{Miscellaneous} \category{See
  \url{http://acm.org/about/class/1998/} for the full list of ACM
  classifiers. This section is required.}{}{}

\section{Introduction}

Asynchronous audio communication (AAC) is rapidly becoming available to general users of technology. 
While text is still by far the most prevalent mode of communication on the Internet, social platforms such as WhatsApp, iMessage, and Facebook provide the option to record audio-only messages in addition to text, photo, and video. 
Audio is desirable in such situations because it allows users to deliver more expressive, nuanced messages.

AAC holds considerable potential for more specialized settings as well, namely collaboration for education and business. 
For instance, audio messages have been favorably used to disseminate feedback on assignments in online education courses \cite{ice,oomen}. 
On the other hand, voice communication is much less prevalent in business environments; the predominant manifestation of audio-only communication has been voicemail, a paradigm that is on the decline and generally regarded as ``onerous'' and ``laborious'' \cite{whittaker}. 
According to Grudin \cite{grudin}, the primary reason for this failure of voicemail is an imbalance of work between speaker and listener: message creators are afforded the convenience of rapid production, while listeners are often faced with a bevy of voice messages which must be processed essentially at the original rate of speech. 
Perhaps for this reason, email continues to predominate over voice messages in corporate communication despite the diminished individuality and, more critically, the higher risk of miscommunication in textual media \cite{byron}.

Despite the difficulties that Grudin points out, AAC is undoubtedly superior to text in certain contexts. 
For instance, audio messages could be used as annotations on virtual documents, enabling the user to consume others' comments aurally without distracting the user visually \cite{yoon}.
Another area of benefit is in online education, where voice communication has been shown to improve student-student and student-instructor engagement and a sense of the instructor's social presence \cite{oomen,tu}. 
However, we hypothesized that lightweight editing tools would be beneficial or even essential to deploying AAC in educational contexts, since many students have trouble articulating their ideas vocally even in physical classrooms.
Therefore, we developed a user-friendly audio production tool which we call SimpleSpeech and applied it to the scenario of discussion on an online forum. 
The mental workload experienced by high school students and teachers recording voice messages was then studied, along with the linguistic features of the messages they produced.

The structure of the paper is as follows: first, earlier works that have attempted to resolve problems with AAC are discussed. 
Next, we discuss the new design and interaction paradigm that make SimpleSpeech accessible to general users, as well as an evaluation of its usability. 
Finally, the mental workload and linguistic aspects of voice messages produced using SimpleSpeech are investigated, providing insight into future applications of AAC for online collaboration.

\section{Related Work}
Past approaches to combined audio and text modalities have their origin in voicemail systems, which exemplify the imbalance between audio production and consumption. 
To remedy this, Whittaker et al. \cite{whittaker} attempted to create a more balanced, usable system by converting the audio messages into text using automatic speech recognition (ASR). 
In that study, the combination of text and audio was considerably more appealing to listeners since it allowed them to extract information from messages more efficiently. 
Most studies have utilized ASR to produce the textual representation of speech since the technique became feasibly accurate.

Other related techniques for audio communication can be found in systems developed for document-based collaboration. 
Elaborating on the findings of earlier platforms for multimedia annotation \cite{bargeron}, for example, an application called RichReview developed by Yoon et al. \cite{yoon} facilitates discussions over PDF documents via text, audio, or handwriting annotations. 
An asynchronous audio solution for these document-based scenarios is especially relevant since it would allow commentators to have richer discussions over the document material without visually distracting from the original content. 

In addition to increasing the efficiency of audio consumption, ASR can also be used to facilitate speech editing. 
Phoneme alignment algorithms based on hidden Markov models, such as the Penn Phonetics Lab Forced Aligner \cite{p2fa}, enable the persistent synchronization of audio and text representations as they are manipulated. 
Creating effective user interface paradigms that synthesize these two modalities, however, is a challenge that has been addressed mostly in dedicated audio and video editing software. 
For instance, a video editor developed by Casares et al. \cite{casares} enables the user to edit either the audio track or its transcript, with changes in one mode reflected in the other. 
A similar approach was utilized by Rubin et al. \cite{rubin}, who recently created a text-based editor for ``audio stories'' containing both speech and music.

In software targeted for non-professional users, Whittaker and Amento \cite{whittaker_semantic} developed a "semantic speech editor" in which the sole means of editing was through the transcription. 
They found that using the transcription as a proxy for audio editing was faster and produced better-quality revisions than audio-only modes, noting that the ability to scan quickly for regions requiring editing obviated the need for repeated playback. 
However, the ASR in their 2004 study suffered from a 28\% error rate; today's language models permit considerably greater accuracy, resulting in potentially even easier and faster audio editing.


\section{Software Design}
Building on the capabilities developed in these prior studies, SimpleSpeech is a web-based graphical interface for recording and editing short voice messages in a discussion setting.
Our design goals were as follows: to utilize live ASR transcription to assist in speech editing, to enable easy audio manipulation on the word level, and to add the more complex features on a tiered basis through modes and quasi-modes.
The appearance of the final SimpleSpeech user interface (UI) is shown in Fig. WHAT.

There are a wide variety of approaches to audio editing, ranging from waveform-only interfaces such as Audacity and Adobe Audition to "semantic speech editors" \cite{whittaker_semantic} which show only a transcript. 
For SimpleSpeech, we decided to adopt an interaction paradigm similar to the latter, allowing the user to edit a textual representation which was time-aligned with the audio.
This choice was made for two reasons: (1) in general, users are much more familiar with text than with waveform editing; and (2) representing the audio as text would greatly increase the efficiency of editing speech on the word level, in contrast to the greater complexity of millisecond-level waveform operations. 

Accordingly, the UI for SimpleSpeech devotes the majority of the editing panel to the transcription. 
Users interact with the text by selecting, editing, and deleting tokens, which are colored blue for recognized words and green for pauses and unrecognized sounds. 
In addition to deletion using the Backspace key, green pause tokens can be inserted or extended using the space bar.
Deleting and inserting tokens results in the appropriate modifications automatically applied to the working audio file.

We did choose to include a waveform visualization as part of the UI, though it serves only the auxiliary purpose of reinforcing to the user that he or she is ultimately manipulating audio, not text. The waveform incorporates several subtle indications of the mapping between its contents and the transcription:
\begin{itemize}
	\item The waveform for the line of text currently containing the caret is rendered.
	\item The time-aligned word boundaries are displayed as vertical lines.
	\item If one or more tokens are selected, the audio segment corresponding to the selection is highlighted.
	\item Deletions and pause insertions are visualized through a collapse/expand animation.
\end{itemize}
We found the presence of a waveform to be an important visual indicator of the purpose of SimpleSpeech.
Without the waveform, users' inclination was to disregard the original speech and use the system as a dictation tool.

\subsection{Simplification through Modes and Quasi-Modes}
Our use of text as a proxy for editing audio rendered it necessary to clearly delineate the capabilities of SimpleSpeech in comparison to a word processor.
For instace, direct text input is disallowed in the transcript area to avoid inserting words not present in the original recording.
The solid appearance and inability to move the caret within the tokens visually confirms that the transcript cannot be edited without correspondence to the audio.
However, we found the capability to edit individual words in the transcript to be desirable, especially in the case of ASR errors.
This transcription editing functionality is available in a separate mode, accessed by pressing the Return key, and applies only to single words to avoid undermining the cohesiveness of the tokens. 

During pilot testing the need arose for a fast way to play back and pause the audio; however, the conventional keyboard shortcut for playback, the spacebar, was already in use for the pause insertion feature.
We resolved this problem by using Shift+space for playing and pausing.
In effect, the playback functionality was encapsulated as a \emph{quasi-mode}, a set of distinct features that are accessed while performing a constant action (in this case, pressing the Shift key) \cite{raskin}. 
The modal design helps prevent basic or beginning users from being visually overwhelmed, while allowing more advanced users rapid access to the higher-level features.

\subsection{Implementation}
Our text-based approach requires a reliable transcription as well as time intervals corresponding to each word, both of which are provided by the IBM Watson Developer Cloud speech-to-text transcription service.
The web application is written in JavaScript and communicates through a web socket with the IBM server for transcription.
Editing is accomplished by maintaining a data model consisting of one or more user-created audio resources as well as a list of timestamps, where each timestamp links a token in the text area to a specific time interval within an audio resource. 
When the user plays back the message, the data model ``renders'' a complete audio recording by stitching together the audio from each timestamp. 

\subsection{Design Process}
We followed an iterative procedure to develop SimpleSpeech, beginning with the design goals mentioned above.
The early stages of the application mainly focused on incorporating basic features, such as recording, playback and basic word manipulations. 
After building an initial prototype of the application, an informal pilot test was conducted with 5 participants. 
Each user was given a brief introduction to the software and shown how to use the basic features, then given the scenario of creating an audio response to a fellow student's textual claim on an online forum. 
(The prompts used in all tests were adapted from the GRE Pool of Issue Topics.) 
After using the software, users were interviewed to obtain feedback on the prototype, yielding the following modifications:

\emph{Tokenization}. 
Initially, the words were tokenized so that words and pauses could be deleted in their entirety, but the user could still edit the contents of the tokens by navigating with the arrow keys.
However, the pilot study participants were confused by the inline transcript editing behavior; they either tried to insert new unrecorded content by typing or, in the opposite extreme, used the recording insert feature even for minor transcription errors. 
We endeavored to clarify these delineations in the next iteration by disabling alphanumeric input to the transcript view entirely and changing the transcription editing functionality to a modal interaction.

\emph{Pause manipulation}.
Another important finding in the pilot study was the importance of being able to introduce and adjust gaps between words, not just to remove them. 
These pauses help punctuate claims and make natural-sounding cuts between audio clips, with the pause length determined by the speaker's preference and the textual context around the gap (e.g., the end of a sentence). 
The original system only allowed the user to delete pauses, so we added a spacebar action to insert a zero audio signal or fragment of silence from the original audio resource into the rendered message. 

\section{Qualitative Evaluation}

The interaction paradigm of SimpleSpeech was tested in a qualitative assessment to determine (1) the practicability of a lightweight text-based audio editor, (2) the effects of minor transcription errors on audio consumption and production, and (3) the implications of being able to edit audio in an asynchronous online discussion.

Participants were introduced to the functionality of the system, then given two untimed tasks. 
First, to simulate an asynchronous audio discussion, the test users were asked to read an audio comment left by the previous tester and create an audio response. 
Next, they received a different, textual prompt and created an audio comment which would be consumed by the next user. 
In both cases the user was asked to edit their recordings to be polished and clear.
The participants were interviewed at the end of the test over three general topics: (1) comparing the hybrid editor with conventional text editing, (2) comparing the discussion component with a text-based or face-to-face conversation, and (3) any user experience issues that occurred during the study. 
Afterwards, the interviews were transcribed, conversational elements filtered out, and the remaining sentences analyzed via two-step coding (open coding followed by flat coding). 

The sample for the study consisted of 9 test subjects (4 male, 5 female, henceforth denoted $P_1, P_2, \ldots, P_9$). 
All participants were native English speakers. 
Two individuals, $P_2$ and $P_3$, were professional media editors who provided technical feedback and a comparison to pure audio editing; the remainder were interns and high school students.

\subsection{Results}

Most non-professional users felt SimpleSpeech gave them ``plenty of control'' over the editing process ($P_4,\,P_5,\,P_6,\,P_8$). 
The professional editors did note that most people in their field would not find this software adequate for their needs; but, as $P_2$ conceded, the intended market users ``don't have to play with the settings which is why they don't use a professional audio editor.'' 
With respect to the style of user edits, the most commonly-used editing tool was the removal of disfluencies ($P_1,\,P_2,\,P_4,\,P_5,\,P_7$), followed by space deletion ($P_2,\,P_3,\,P_5,\,P_6,\,P_8$). 
Only $P_1$ and $P_8$ edited large chunks of audio by deleting or rerecording, and $P_8$ reported doing so only to improve the smoothness of a smaller change in a sentence.
Most participants characterized this editing experience as being a text-focused one, suggesting that the translation to text was in fact a useful proxy for editing audio. 
The text modality was described as ``more accessible, more doable'' than pure waveform editing, which could be ``scary for people who don't do video stuff'' ($P_3,\,P_7$).

In many cases, the transcription proved to be an essential element of both the production and the consumption interfaces. 
To determine the effect of errors in the textual representation, the previous participants' comments were displayed to users with an unedited ASR transcript instead of the polished, human-edited one. 
Despite the occasional errors, users still found the ASR transcription to be helpful in allowing them to ``see all the points they were making instead of having to remember them'' ($P_4,\,P_6$). 
For some users, the transcription caused no problems in comprehension, while others experienced errors that required them to pay more attention to the audio ($P_8$). 
On the whole, ASR succeeded in ``getting the basic idea across'' ($P_3$) but could not stand alone without the original recording. 
In the editing scenario, most users agreed that the transcription functionality was ``overall quite accurate'' if they spoke clearly enough ($P_1,\,P_3,\,P_4,\,P_5,\,P_6,\,P_7,\,P_9$). 

In some participants we also observed pressure associated with the audio production process. $P_4,\,P_7,$ and $P_9$ described a ``psychological sort of ... need to get it all out, and the fact that it won't necessarily be as organized there.'' 
Another tester, $P_5$, had ``a tendency to get like a blank slate'' in which he ``couldn't think of anything to say.'' 
The elevated mental task load that $P_5$ describes could be inherent in oral discussion; $P_9$ noted that ``[it] might just be the fact that I was recording,'' and in fact ``editing would make it nicer.'' 
Because this phenomenon was present despite the ability to edit and because it has not been explored in the literature to our knowledge, we decided to analyze the task load aspect of using SimpleSpeech in the quantitative study.

Interestingly, the participants' awareness of their audience and the ability to edit tended to drive up the quality of recordings.
Four users mentioned the formality of recordings using the software of their own volition or prompted by a question about pressure ($P_1,\,P_5,\,P_7,\,P_9$). 
$P_8$ described the situation as ``an expectation'' to edit, given that ``I know that I've had that opportunity and someone else would know that I had that opportunity.'' 
The speakers' inclination to consider their listeners is exemplified by $P_9$, when asked why she was motivated to edit her messages:
\begin{quote}
	Personally I'm editing to express myself a little more in a polished way when I'm writing.... especially if I know someone else is going to review it and be able to respond, I want to make sure I'm as clear as possible and as concise in a way that doesn’t really come across when I'm talking.
\end{quote}
Listening to another participant before initiating their own comment was likely a factor in determining the users' performance, since the exposure ``gave ... an understanding of how long of a comment, or what kind of direction people were trying to take the discussion'' ($P_9$). 
Editing contributed to the increased quality as well: ``Since you have the ability to edit things, it feels like you're talking to somebody who's prepared a point or a conversational view'' ($P_5$). 
We chose to explore this phenomenon quantitatively to determine if it was real or simply perceived by the speakers, and how it was affected by the ability to edit.

\section{Quantitative Evaluation}


\begin{table}
  \centering
  \begin{tabular}{r c c}
    \toprule
    & \multicolumn{2}{c}{\small{\textbf{Caption}}} \\
    \cmidrule(r){2-3}
    {\small\textbf{Objects}}
    & {\small \textit{Pre-2002}}
    & {\small \textit{Current}} \\
    \midrule
    Tables & Above & Below \\
    Figures & Below & Below \\
    \bottomrule
  \end{tabular}
  \caption{Table captions should be placed below the table. We
    recommend table lines be 1 point, 25\% black. Minimize use of
    unnecessary table lines.}~\label{tab:table1}
\end{table}

\section{Sections}

The heading of a section should be in Helvetica or Arial 9-point bold,
all in capitals. Sections should \textit{not} be numbered.

\subsection{Subsections}

Headings of subsections should be in Helvetica or Arial 9-point bold
with initial letters capitalized.  For sub-sections and
sub-subsections, a word like \emph{the} or \emph{of} is not
capitalized unless it is the first word of the heading.

\subsubsection{Sub-subsections}

Headings for sub-subsections should be in Helvetica or Arial 9-point
italic with initial letters capitalized.  Standard
\texttt{{\textbackslash}section}, \texttt{{\textbackslash}subsection},
and \texttt{{\textbackslash}subsubsection} commands will work fine in
this template.

\section{Figures/Captions}

Place figures and tables at the top or bottom of the appropriate
column or columns, on the same page as the relevant text (see
Figure~\ref{fig:figure1}). A figure or table may extend across both
columns to a maximum width of 17.78 cm (7 in.).

\begin{figure*}
  \centering
  \includegraphics[width=2\columnwidth]{figures/map}
  \caption{In this image, the map maximizes use of space. You can make
    figures as wide as you need, up to a maximum of the full width of
    both columns. Note that \LaTeX\ tends to render large figures on a
    dedicated page. Image: \ccbynd~ayman on
    Flickr.}~\label{fig:figure2}
\end{figure*}

Captions should be Times New Roman or Times Roman 9-point bold.  They
should be numbered (e.g., ``Table~\ref{tab:table1}'' or
``Figure~\ref{fig:figure1}''), centered and placed beneath the figure
or table.  Please note that the words ``Figure'' and ``Table'' should
be spelled out (e.g., ``Figure'' rather than ``Fig.'') wherever they
occur. Figures, like Figure~\ref{fig:figure2}, may span columns and
all figures should also include alt text for improved accessibility.
Papers and notes may use color figures, which are included in the page
limit; the figures must be usable when printed in black-and-white in
the proceedings.

The paper may be accompanied by a short video figure up to five
minutes in length. However, the paper should stand on its own without
the video figure, as the video may not be available to everyone who
reads the paper.  

\subsection{Inserting Images}
When possible, include a vector formatted graphic (i.e. PDF or EPS).
When including bitmaps,  use an image editing tool to resize the image
at the appropriate printing resolution (usually 300 dpi).

\section{Language, Style and Content}

The written and spoken language of SIGCHI is English. Spelling and
punctuation may use any dialect of English (e.g., British, Canadian,
US, etc.) provided this is done consis- tently. Hyphenation is
optional. To ensure suitability for an international audience, please
pay attention to the following:

\begin{itemize}
\item Write in a straightforward style.
\item Try to avoid long or complex sentence structures.
\item Briefly define or explain all technical terms that may be
  unfamiliar to readers.
\item Explain all acronyms the first time they are used in your
  text---e.g., ``Digital Signal Processing (DSP)''.
\item Explain local references (e.g., not everyone knows all city
  names in a particular country).
\item Explain ``insider'' comments. Ensure that your whole audience
  understands any reference whose meaning you do not describe (e.g.,
  do not assume that everyone has used a Macintosh or a particular
  application).
\item Explain colloquial language and puns. Understanding phrases like
  ``red herring'' may require a local knowledge of English.  Humor and
  irony are difficult to translate.
\item Use unambiguous forms for culturally localized concepts, such as
  times, dates, currencies, and numbers (e.g., ``1--5--97'' or
  ``5/1/97'' may mean 5 January or 1 May, and ``seven o'clock'' may
  mean 7:00 am or 19:00). For currencies, indicate equivalences:
  ``Participants were paid {\fontfamily{txr}\selectfont \textwon}
  25,000, or roughly US \$22.''
\item Be careful with the use of gender-specific pronouns (he, she)
  and other gendered words (chairman, manpower, man-months). Use
  inclusive language that is gender-neutral (e.g., she or he, they,
  s/he, chair, staff, staff-hours, person-years). See the
  \textit{Guidelines for Bias-Free Writing} for further advice and
  examples regarding gender and other personal
  attributes~\cite{Schwartz:1995:GBF}. Be particularly aware of
  considerations around writing about people with disabilities.
\item If possible, use the full (extended) alphabetic character set
  for names of persons, institutions, and places (e.g.,
  Gr{\o}nb{\ae}k, Lafreni\'ere, S\'anchez, Nguy\~{\^{e}}n,
  Universit{\"a}t, Wei{\ss}enbach, Z{\"u}llighoven, \r{A}rhus, etc.).
  These characters are already included in most versions and variants
  of Times, Helvetica, and Arial fonts.
\end{itemize}

\section{Accessibility}
The Executive Council of SIGCHI has committed to making SIGCHI
conferences more inclusive for researchers, practitioners, and
educators with disabilities. As a part of this goal, the all authors
are asked to work on improving the accessibility of their
submissions. Specifically, we encourage authors to carry out the
following five steps:
\begin{enumerate}
\item Add alternative text to all figures
\item Mark table headings
\item Add tags to the PDF
\item Verify the default language
\item Set the tab order to ``Use Document Structure''
\end{enumerate}
For more information and links to instructions and resources, please
see: \url{http://chi2016.acm.org/accessibility}.  The
\texttt{{\textbackslash}hyperref} package allows you to create well tagged PDF files,
please see the preamble of this template for an example.

\section{Page Numbering, Headers and Footers}
Your final submission should not contain footer or header information
at the top or bottom of each page. Specifically, your final submission
should not include page numbers. Initial submissions may include page
numbers, but these must be removed for camera-ready. Page numbers will
be added to the PDF when the proceedings are assembled.

\section{Producing and Testing PDF Files}

We recommend that you produce a PDF version of your submission well
before the final deadline.  Your PDF file must be ACM DL
Compliant. The requirements for an ACM Compliant PDF are available at:
{\url{http://www.sheridanprinting.com/typedept/ACM-distilling-settings.htm}}.

Test your PDF file by viewing or printing it with the same software we
will use when we receive it, Adobe Acrobat Reader Version 10. This is
widely available at no cost. Note that most
reviewers will use a North American/European version of Acrobat
reader, so please check your PDF accordingly.

When creating your PDF from Word, ensure that you generate a tagged
PDF from improved accessibility. This can be done by using the Adobe
PDF add-in, also called PDFMaker. Select Acrobat | Preferences from
the ribbon and ensure that ``Enable Accessibility and Reflow with
tagged Adobe PDF'' is selected. You can then generate a tagged PDF by
selecting ``Create PDF'' from the Acrobat ribbon.

\section{Conclusion}

It is important that you write for the SIGCHI audience. Please read
previous years’ proceedings to understand the writing style and
conventions that successful authors have used. It is particularly
important that you state clearly what you have done, not merely what
you plan to do, and explain how your work is different from previously
published work, i.e., the unique contribution that your work makes to
the field. Please consider what the reader will learn from your
submission, and how they will find your work useful. If you write with
these questions in mind, your work is more likely to be successful,
both in being accepted into the conference, and in influencing the
work of our field.

\section{Acknowledgments}

Sample text: We thank all the volunteers, and all publications support
and staff, who wrote and provided helpful comments on previous
versions of this document. Authors 1, 2, and 3 gratefully acknowledge
the grant from NSF (\#1234--2012--ABC). \textit{This whole paragraph is
  just an example.}

% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
\balance{}

\section{References Format}
Your references should be published materials accessible to the
public. Internal technical reports may be cited only if they are
easily accessible (i.e., you provide the address for obtaining the
report within your citation) and may be obtained by any reader for a
nominal fee. Proprietary information may not be cited. Private
communications should be acknowledged in the main text, not referenced
(e.g., ``[Golovchinsky, personal communication]'').

Use a numbered list of references at the end of the article, ordered
alphabetically by first author, and referenced by numbers in
brackets~\cite{ethics,Klemmer:2002:WSC:503376.503378}. For papers from
conference proceedings, include the title of the paper and an
abbreviated name of the conference (e.g., for Interact 2003
proceedings, use Proc.\ Interact 2003). Do not include the location of
the conference or the exact date; do include the page numbers if
available. See the examples of citations at the end of this document
and in the accompanying \texttt{BibTeX} document.

References \textit{must be the same font size as other body
  text}. References should be in alphabetical order by last name of
first author. Example reference formatting for individual journal
articles~\cite{ethics}, articles in conference
proceedings~\cite{Klemmer:2002:WSC:503376.503378},
books~\cite{Schwartz:1995:GBF}, theses~\cite{sutherland:sketchpad},
book chapters~\cite{winner:politics}, a journal issue~\cite{kaye:puc},
websites~\cite{acm_categories,cavender:writing},
tweets~\cite{CHINOSAUR:venue}, patents~\cite{heilig:sensorama}, and
online videos~\cite{psy:gangnam} is given here. This formatting is a
slightly abbreviated version of the format automatically generated by
the ACM Digital Library (\url{http://dl.acm.org}) as ``ACM Ref''. More
details of reference formatting are available at:
\url{http://www.acm.org/publications/submissions/latex_style}.

% REFERENCES FORMAT
% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{sample}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
